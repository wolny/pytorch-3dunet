# %% markdown
# # Image Classification using a convolutional neural network
# %% codecell
# Uncomment and run the commands below if imports fail
# !conda install numpy pytorch torchvision cpuonly -c pytorch -y
# !pip install matplotlib --upgrade --quiet
!pip install jovian --upgrade --quiet
# %% codecell
# Imports
import os
import numpy as np
import torch
import jovian
import matplotlib
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split, Subset
import torchvision
from torchvision.utils import make_grid
import torchvision.transforms as tt
from torchvision.datasets import CIFAR10, ImageFolder
from torchvision.datasets.utils import download_url

import tarfile

# Use a white background for matplotlib figures
matplotlib.rcParams['figure.facecolor'] = '#ffffff'
%matplotlib inline

%config Completer.use_jedi = False
# %% codecell
project_name='CNN-template'
# %% markdown
# ## Download Dataset
# %% codecell
# download dataset from torch repo (will be shuffled)
dataset = CIFAR10(root='data/', train=True, download=True, transform=tt.ToTensor())

dataset_test = CIFAR10(root='data/', train=False, download=True, transform=tt.ToTensor())
# %% codecell
# alternatively download dataset from url
ds_url = "https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz"
download_url(ds_url, '.')
# %% codecell
# Extract from archive save to folder
with tarfile.open('./cifar10.tgz', 'r:gz') as tar:
    tar.extractall(path='./data_url')
# %% codecell
# define a list of classes from dir structure (1 folder per class)
data_url_dir = './data_url/cifar10'
os.listdir(data_url_dir)
classes_url = os.listdir(data_url_dir + '/train')
print(classes_url)
# %% codecell
# look into folder
airplane_files = os.listdir(data_url_dir + '/train/airplane')
print('N images in folder:', len(airplane_files))
print(airplane_files[0:5])
# %% markdown
# ## Prepare dataset
# %% codecell
# calculate sample mean and std deviation
ds = ImageFolder(data_url_dir+'/train', transform=tt.ToTensor())

dl = DataLoader(ds, batch_size=50000)

for i,j in dl:
    ds_mean = i.permute(1,0,2,3).mean(dim=(1,2,3))
    ds_std = i.permute(1,0,2,3).std(dim=(1,2,3))
    print(ds_std)
    print(ds_mean)
    break
# %% codecell
# Data transforms (normalization & data augmentation)
stats = ((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
train_trf = tt.Compose([   tt.RandomCrop(32, padding=4, padding_mode='reflect'),
                           tt.RandomHorizontalFlip(),
                         # tt.RandomRotation(degrees=90),
                         # tt.RandomResizedCrop(32, scale=(0.5,0.9), ratio=(1, 1)),
                         # tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
                           tt.ToTensor(),
                           tt.Normalize(*stats,inplace=True)
                        ])
val_test_trf = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])
# %% codecell
# split in training and validation datasets with different transformations
# define pytorch datasets from folder (won't be shuffled), and transform
dataset_url_traintf = ImageFolder(data_url_dir+'/train',
                          transform=train_trf)
dataset_url_valtf = ImageFolder(data_url_dir+'/train',
                          transform=val_test_trf)
dataset_url_test = ImageFolder(data_url_dir+'/test',
                          transform=val_test_trf)
# %% codecell
# generate indices of training and validation datasets
idx = list(range(len(dataset_url_traintf)))
np.random.shuffle(idx)
idx_train = idx[:40000]
idx_val = idx[40000:]

# define training and validation datasets
dataset_url_train = Subset(dataset_url_traintf, idx_train)
dataset_url_val = Subset(dataset_url_valtf, idx_val)
# %% codecell
# sample object types (tuple of tensor and int), sizes and ds classes
img, label = dataset_url_train[5000]
print('Object types:', type(img), type(label))
print('Sizes:', img.shape, label)
print('Dataset slasses:', dataset_url_traintf.classes)
# %% codecell
# show one sample
def show_sample(data):
    ''' in: tuple(torch.tensor, int)
        out: sample label and image
    '''
    img, lbl = data
    plt.imshow(img.permute(1,2,0))
    print('Label:', dataset.classes[lbl], '('+str(lbl)+')')
# %% codecell
show_sample(dataset_url_test[5000])
# %% markdown
# ## Training and validation datasets
# %% codecell
# split in training and validation datasets with same transformation
# set ds parameters
val_size = 5000
train_size = len(dataset) - val_size
seed = 42
batch_size = 100

# split in train and val datasets at fixed random seed
train_ds, val_ds = random_split(dataset, [train_size, val_size],
                               generator=torch.Generator().manual_seed(seed))

# define loader objects
train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)
test_loader = DataLoader(dataset_test, batch_size, shuffle=True, num_workers=4, pin_memory=True)
# %% codecell
# define loader objects for training and validation datasets with different transformations
batch_size = 100

train_loader = DataLoader(dataset_url_train,
                          batch_size,
                          shuffle=True,
                          num_workers=4,
                          pin_memory=True)
val_loader = DataLoader(dataset_url_val,
                          batch_size,
                          shuffle=True,
                          num_workers=4,
                          pin_memory=True)
test_loader = DataLoader(dataset_url_test,
                          batch_size,
                          shuffle=True,
                          num_workers=4,
                          pin_memory=True)

# %% codecell
def denormalize(images, means, stds):
    means = torch.tensor(means).reshape(1, 3, 1, 1)
    stds = torch.tensor(stds).reshape(1, 3, 1, 1)
    return images * stds + means

def show_batch(dl):
    for imgs, lbls in dl:
        plt.figure(figsize=(15, 15))
        plt.axis('off')
        denorm_images = denormalize(imgs, *stats)
        plt.imshow(make_grid(denorm_images[:64], nrow=8).permute(1,2,0))
        break
# %% codecell
show_batch(val_loader)
# %% markdown
# ## Models
# %% codecell
# define a network base class with methods common to most networks
class Network_base(nn.Module):

    # pass forward training data, record loss
    def training_step(self, batch):
        inpt, target = batch                     # split in&out per batch
        out = self(inpt)                         # call self.forward, generate predictions
        loss = F.cross_entropy(out, target)      # calculate loss
        return loss

    # pass forward validation data, record metrics
    def validation_batch(self, batch):
        inpt, target = batch                     # split per batch input, target
        out = self(inpt)                         # generate predictions
        loss = F.cross_entropy(out, target)      # calculate batch loss
        accu = self.accuracy(out, target)        # calculate accuracy
        return {'val_loss' : loss.detach(),          # return dict()
                'accuracy' : accu.detach()}

    def validation_epoch(self, result):
        losses_batch = [r['val_loss'] for r in result]      # extract values from dict()
        accu_batch = [r['accuracy'] for r in result]
        loss_epoch = torch.stack(losses_batch).mean()   # calulate epoch mean
        accu_epoch = torch.stack(accu_batch).mean()
        return {'val_loss' : loss_epoch.item(), 'accuracy' : accu_epoch.item()}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], Last learning rate: {:,.5f}, Training loss: {:.4f}, Validation loss: {:.4f}, Accuracy: {:.4f}".format(
               epoch,
               result['lrs'][-1],
               result['train_loss'],
               result['val_loss'],
               result['accuracy']))

    def accuracy(self, output, target):
        _, preds = torch.max(output, dim=1)    # output max value index
        acc = torch.tensor(torch.sum(preds == target).item()/len(preds)) #calc accuracy
        return acc

    def evaluate(self, val_loader):
        result = [self.validation_batch(batch)  # calulate batch mean l&a
                   for batch in val_loader]       # iterate over batches
        return self.validation_epoch(result)    # calculate epoch mean loss&accuray
# %% codecell
# define a CNN model with a specific architecture using network base
class CNN_Model(Network_base):
    def __init__(self):
        super().__init__()
        self.network = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2), # output: 32 x 16 x 16 (channel x image size)

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4

            nn.Flatten(),
            nn.Linear(256*4*4, 1024),  # input: 2048, output: 1024
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 10))

    def forward(self, xb):
        return self.network(xb)
# %% codecell
# model instance
model = CNN_Model()
# %% codecell
# CNN testrun
for imgs, lbls in train_loader:
    print(imgs.shape)
    out = model(imgs)
    print(out.shape)
    break
# %% codecell
# define a network with a residual architecture
class ResNet9(Network_base):
    def __init__(self, in_channels, n_classes):
        super().__init__()

        self.conv1 = self.conv_block(in_channels, 64)         # 3 x 32 x 32 -> 64 x 32 x 32
        self.conv2 = self.conv_block(64, 128, pool=True)      # -> 128 x 16 x 16

        self.res1 = nn.Sequential(self.conv_block(128, 128), self.conv_block(128, 128))

        self.conv3 = self.conv_block(128, 256, pool=True)     # -> 256 x 8 x 8
        self.conv4 = self.conv_block(256, 512, pool=True)     # -> 512 4 x 4

        self.res2 = nn.Sequential(self.conv_block(512, 512), self.conv_block(512, 512))

        self.classifier = nn.Sequential(nn.MaxPool2d(4), # -> 512 x 1 x 1
                                        nn.Flatten(),
                                        nn.Dropout(0.2),
                                        nn.Linear(512, n_classes))

    def conv_block(self, in_channels, out_channels, pool=False):
        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                  nn.BatchNorm2d(out_channels),
                  nn.ReLU(inplace=True)]
        if pool: layers.append(nn.MaxPool2d(2))
        return nn.Sequential(*layers)

    def forward(self, xb):
        out = self.conv1(xb)
        out = self.conv2(out)
        out = self.res1(out) + out        # sum x and residual
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.res2(out) + out        # sum x and residual
        out = self.classifier(out)
        return out
# %% codecell
model_resnet = ResNet9(3, 10)
# %% markdown
# ## Transfer data & model to GPU
# %% codecell
def get_default_device():
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

# move data to GPU memory
def to_device(data, device):
    if isinstance(data, (list, tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

# load data batchwise to GPU if called
class DeviceDataLoader():
    def __init__(self, dataloader, device):
        self.dataloader = dataloader
        self.device = device

    def __iter__(self):
        for batch in self.dataloader:
            yield to_device(batch, self.device)

    def __len__(self):
        return len(self.dataloader)
# %% codecell
# set default device to GPU if applicable
device = get_default_device()
device
# %% markdown
# ## Training
# %% codecell
# fit the model
def fit(epochs, learning_rate, model, train_loader, val_loader, opt_funct=torch.optim.SGD):
    optimizer=opt_funct(model.parameters(), learning_rate)
    history = []

    for epoch in range(epochs):                  # iterate n_epochs
        train_losses = []                        # record training losses
        for batch in train_loader:               # iterate over batches
            loss = model.training_step(batch)    # generate predictions # Calculate loss
            train_losses.append(loss)            # record training loss
            loss.backward()                      # compute gradients
            optimizer.step()                     # update weights
            optimizer.zero_grad()                # reset gradients

        result = model.evaluate(val_loader)     # compute validation l&a
        result['train_loss'] = torch.stack(train_losses).mean().item()
        model.epoch_end(epoch, result)          # print epoch l&a
        history.append(result)                  # log epoch l&a to list
    return history
# %% codecell
# transfer data to GPU
train_loader = DeviceDataLoader(train_loader, device)
val_loader = DeviceDataLoader(val_loader, device)
test_loader = DeviceDataLoader(test_loader, device)

# transfer model to GPU
to_device(model, device)

# check
for batch in train_loader:
    img, lbl = batch
    print(img.device)
    break
# %% codecell
# model pre training performance
history = [model.evaluate(val_loader)]
print(history)
# %% codecell
# set hyperparameters
learning_rate = 1.0e-3
epochs = 10

# set optimisation function
opt_function = torch.optim.Adam
# %% codecell
# Train and record history
history += fit(epochs, learning_rate, model, train_loader, val_loader,
               opt_funct=opt_function)
# %% codecell
# plot metrics
def plot_metrics(history):
    accuracies = [h['Accuracy'] for h in history]
    train_losses = [h['train_loss'] for h in history[1:]]
    val_losses = [h['Loss'] for h in history]

    plt.figure(figsize=(10, 4))
    plt.subplot(121)
    plt.plot(accuracies, '-x')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.subplot(122)
    plt.plot(range(1, len(train_losses)+1, 1),
             train_losses, '-rx',label='training')
    plt.plot(val_losses, '-bx', label='validation')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.suptitle('Accuracy and loss vs. No of epochs')
    plt.legend()
    plt.show()
# %% codecell
plot_metrics(history)
# %% codecell
# test dataset l&a
performance = model.evaluate(test_loader)
print(performance['Accuracy'])
# %% markdown
# ## Prediction
# %% codecell
def predict_image(img, model):
    xb = to_device(img.unsqueeze(0), device)
    yb = model(xb)
    _, preds  = torch.max(yb, dim=1)
    return preds[0].item()
# %% codecell
n=10
img, label = dataset_test[n]
plt.imshow(img.permute(1, 2, 0))
print('Label:', dataset.classes[label], ', Predicted:', dataset.classes[predict_image(img, model)])
# %% markdown
# ## Learning from the model
# %% codecell
# plot a single kernel
kernel = model.network[0].weight.view(-1, 3, 3)[95].detach().cpu()
plt.imshow(kernel, cmap='gray')
plt.colorbar()
plt.show()
# %% codecell
# define a truncated model using the learned prms yielding intermediary output
class layer(nn.Module):
    def __init__(self):
        super(layer, self).__init__()
        self.network = nn.Sequential(
            *list(model.network.children())[:17]
        )
    def forward(self, xb):
        out = self.network(xb)
        return out
# %% codecell
model2 = to_device(layer(), device)
# %% codecell
model2.parameters
# %% codecell
# get intermediary output & labels
out = []
lbls = []
for batch in val_loader:
    xb, labels = batch
    yb = model2(xb)
    out.append(yb.detach().cpu())
    lbls.append(labels.cpu())

out = torch.stack(out)
lbls = torch.stack(lbls)
# %% codecell
# plot intermed. output
img_inter = out.reshape(-1, 1024)[6]

plt.imshow(img_inter.reshape(32, 32), cmap='jet')
plt.colorbar()
plt.show()
# %% codecell
# pca on intermed output
from sklearn import manifold, decomposition
# %% codecell
data_inter = out.reshape(-1, 1024)[0:1000].numpy()
data_lbls = lbls.reshape(-1).numpy()[0:1000]
# %% codecell
pca = decomposition.PCA(n_components=2)
data_pca = pca.fit_transform(data_inter)
# %% codecell
plt.figure(figsize=(10,8))
plt.scatter(data_red[:, 0], data_pca[:, 1], c=data_lbls, cmap=plt.cm.get_cmap('Spectral', 10))
plt.colorbar(ticks=range(10))
plt.clim(-0.5, 9.5)
plt.show()
# %% codecell
# prepare data for tsne and umap
pca = decomposition.PCA(n_components=50)
data_red = pca.fit_transform(data_inter)
# %% codecell
# tsne on intermed putput
tsne = manifold.TSNE(n_components=2, init='pca', random_state=42)
Y_tsne = tsne.fit_transform(data_red)
# %% codecell
Y_tsne[0]
# %% codecell
plt.figure(figsize=(10,8))
plt.scatter(Y_tsne[:, 0], Y_tsne[:, 1], c=data_lbls, cmap=plt.cm.get_cmap('Spectral', 10))
plt.colorbar(ticks=range(10))
plt.clim(-0.5, 9.5)
plt.show()
# %% codecell
# umap on intermediary output
import umap
# %% codecell
umap_red = umap.UMAP(n_components=2, init='spectral',random_state=42)
Y_umap = umap_red.fit_transform(data_red)
# %% codecell
plt.figure(figsize=(10,8))
plt.scatter(Y_umap[:, 0], Y_umap[:, 1], c=lbls.reshape(-1).numpy()[0:1000], cmap=plt.cm.get_cmap('Spectral', 10))
plt.colorbar(ticks=range(10))
plt.clim(-0.5, 9.5)
plt.show()
# %% markdown
# ## Saving and loading the model
# %% codecell
# save current model parameters
torch.save(model.state_dict(), 'CIFAR10-cnn.pth')
# %% codecell
# load model parameters
model2 = CIFAR10CnnModel()
model2.load_state_dict(torch.load('CIFAR10-cnn.pth'))
# %% markdown
# ## Uploading notebook and saving metrics
# %% codecell
jovian.commit(project=project_name, environment='auto', outputs=['CIFAR10-cnn.pth'])
jovian.commit(project=project_name, environment='auto', outputs=['CIFAR10-cnn.pth']) # Kaggle commit fails sometimes, so try again..
# %% codecell
architecture='afsdfasd'
jovian.log_hyperparams(architecture=architecture,
                       batch_size=batch_size,
                       learning_rate=learning_rate,
                       epochs=epochs,
                       optimizer=str(opt_function))
# %% codecell
jovian.log_metrics(training_loss=history[-1]['train_loss'],
                   validation_loss=history[-1]['Loss'],
                   test_loss=performance['Loss'],
                   validation_accuracy=history[-1]['Accuracy'],
                   test_accuracy=performance['Accuracy'])
